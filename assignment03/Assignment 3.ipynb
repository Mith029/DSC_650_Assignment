{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import genson\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "# endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data(endpoint_path='/Users/mithilpatel/Desktop/DSC_650/'):\n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     anon=True,\n",
    "    #     client_kwargs={\n",
    "    #         'endpoint_url': endpoint_url\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    src_data_path = endpoint_path + 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        records = [json.loads(line) for line in f.readlines()]\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    \n",
    "    # Building Schema using Genson library\n",
    "    def create_schema(records):\n",
    "        builder = genson.SchemaBuilder()\n",
    "        for i in records:\n",
    "            builder.add_object(i)\n",
    "        schema = builder.to_schema()\n",
    "\n",
    "        # Dumping records to json\n",
    "        schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "        with open(schema_path,'w') as f:\n",
    "            json.dump(schema, f, indent=2)\n",
    "            \n",
    "    # Calling the function to create a json file for schema (Uncommend to create one)      \n",
    "    # create_schema(records)\n",
    "    \n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "        \n",
    "    with open(\"results/schema_validation.txt\", 'w') as f:    \n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                jsonschema.validate(record, schema)\n",
    "            except jsonschema.exceptions.ValidationError as e:\n",
    "                print(\"Validation failed: \", e)\n",
    "        print(\"Validation successful!\")\n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    \n",
    "    # Obtaining schema-json file\n",
    "    with open(schema_path, 'r') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    # Wrdata_pathg to Avro file\n",
    "    with open(data_path, \"wb\") as f:\n",
    "        fastavro.writer(f, schema, records)\n",
    "    \n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset(endpoint_path='/Users/mithilpatel/Desktop/DSC_650/'):\n",
    "    # s3 = s3fs.S3FileSystem(\n",
    "    #     anon=True,\n",
    "    #     client_kwargs={\n",
    "    #         'endpoint_url': endpoint_url\n",
    "    #     }\n",
    "    # )\n",
    "    \n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    src_data_path = endpoint_path + 'data/processed/openflights/routes.jsonl.gz'\n",
    "    \n",
    "    # Reading the json data and storing into an array\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "    # Saving the data as the Parquet dataset using pyarrow\n",
    "    table = pa.Table.from_pydict({key: [row[key] for row in records] for key in records[0]}, schema=None)\n",
    "    \n",
    "    # Writing the dataset into a table\n",
    "    pq.write_table(table, parquet_output_path)\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS EMPTY? \n",
      "THIS IS EMPTY? \n",
      "THIS IS EMPTY? \n",
      "THIS IS EMPTY? \n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    # Creating an instance of Airport class with no set values\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    \n",
    "    if airline is None: \n",
    "        return None\n",
    "    if airline.airline_id is None:\n",
    "        return\n",
    "    if airline.get('name'):\n",
    "        obj.name = airline.get('name')\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    if airline.get('active'):\n",
    "        obj.active = airline.get('active')\n",
    "        \n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records[:4]:\n",
    "        route = routes_pb2.Route()\n",
    "        route.airline.Cop\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "\n",
    "#         routes.route.append(route)\n",
    "\n",
    "#     data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "#     with open(data_path, 'wb') as f:\n",
    "#         f.write(routes.SerializeToString())\n",
    "        \n",
    "#     compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "#     with open(compressed_path, 'wb') as f:\n",
    "#         f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    \n",
    "    #Creating a geoindex folder to store indexes\n",
    "    geoindex_dir = results_dir.joinpath('geoindex/')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    \n",
    "    for coord in records:\n",
    "        #Checking whether source airport exist\n",
    "        if coord.get('src_airport'):\n",
    "            # Getting the geohash for each source airport & appending to a list\n",
    "            src_airport = coord.get('src_airport')\n",
    "            geohash = pygeohash.encode(src_airport['latitude'], src_airport['longitude'])\n",
    "            hashes.append(geohash) \n",
    "            \n",
    "            # Creating a geohash and source airport key-value pair\n",
    "            output_dic = {}\n",
    "            output_dic[geohash] = coord['src_airport']   \n",
    "            \n",
    "            # Creating folders to store the dictionary \n",
    "            index_folder = geoindex_dir.joinpath(f'{geohash[-3]}/{geohash[-3:-1]}')\n",
    "            index_folder.mkdir(exist_ok=True,parents=True)\n",
    "            os.chdir(index_folder)\n",
    "            \n",
    "            # Writing the geohash as the key and src_airport as the value into each files\n",
    "            with open(f'{geohash[-3:]}.jsonl.gz','wb') as f:\n",
    "                f.write(json.dumps(output_dic).encode(\"utf-8\"))\n",
    "                f.write(b\"\\n\")  \n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shortest airport is Eppley Airfield at 19.545000 km'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    input_geohash = pygeohash.encode(41.1499988,-95.91779, precision=5)\n",
    "    shortest_dist = 1e100\n",
    "    airport_name = ''\n",
    "    for rec in records:\n",
    "        if rec.get('src_airport'):\n",
    "            src_airport = rec.get('src_airport')\n",
    "            geohash = pygeohash.encode(src_airport['latitude'], src_airport['longitude'])\n",
    "            src_input_dist = pygeohash.geohash_approximate_distance(input_geohash,geohash)/1000\n",
    "            if src_input_dist < shortest_dist:\n",
    "                airport_name = src_airport['name']\n",
    "                shortest_dist = src_input_dist\n",
    "\n",
    "    return f\"Shortest airport is {airport_name} at {shortest_dist:2f} km\"\n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
